{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gwo9bpaVgxXF"
   },
   "source": [
    "##Setup\n",
    "\n",
    "You will need to make a copy of this notebook in your Google Drive before you can edit the homework files. You can do so with **File &rarr; Save a copy in Drive**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6CAdiyTKi4Se"
   },
   "outputs": [],
   "source": [
    "#@title mount your Google Drive\n",
    "#@markdown Your work will be stored in a folder called `hw_16831` by default to prevent Colab instance timeouts from deleting your edits.\n",
    "\n",
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BKE5nA1Fgwwy"
   },
   "outputs": [],
   "source": [
    "#@title set up mount symlink\n",
    "\n",
    "DRIVE_PATH = '/content/gdrive/My\\ Drive/hw_16831'\n",
    "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
    "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
    "  %mkdir $DRIVE_PATH\n",
    "\n",
    "## the space in `My Drive` causes some issues,\n",
    "## make a symlink to avoid this\n",
    "SYM_PATH = '/content/hw_16831'\n",
    "if not os.path.exists(SYM_PATH):\n",
    "  !ln -s $DRIVE_PATH $SYM_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FGK4kbpg3iP"
   },
   "outputs": [],
   "source": [
    "#@title apt install requirements\n",
    "\n",
    "#@markdown Run each section with Shift+Enter\n",
    "\n",
    "#@markdown Double-click on section headers to show code.\n",
    "\n",
    "!apt update\n",
    "!apt install -y --no-install-recommends \\\n",
    "        build-essential \\\n",
    "        curl \\\n",
    "        git \\\n",
    "        gnupg2 \\\n",
    "        make \\\n",
    "        cmake \\\n",
    "        ffmpeg \\\n",
    "        swig \\\n",
    "        libz-dev \\\n",
    "        unzip \\\n",
    "        zlib1g-dev \\\n",
    "        libglfw3 \\\n",
    "        libglfw3-dev \\\n",
    "        libxrandr2 \\\n",
    "        libxinerama-dev \\\n",
    "        libxi6 \\\n",
    "        libxcursor-dev \\\n",
    "        libgl1-mesa-dev \\\n",
    "        libgl1-mesa-glx \\\n",
    "        libglew-dev \\\n",
    "        libosmesa6-dev \\\n",
    "        lsb-release \\\n",
    "        ack-grep \\\n",
    "        patchelf \\\n",
    "        wget \\\n",
    "        xpra \\\n",
    "        xserver-xorg-dev \\\n",
    "        xvfb \\\n",
    "        python3-opengl \\\n",
    "        ffmpeg > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftV6HiHza3E-"
   },
   "outputs": [],
   "source": [
    "#@title clone homework repo\n",
    "#@markdown Note that this is the same codebase from homework 1,\n",
    "#@markdown so you may need to move your old `hw_16831`\n",
    "#@markdown folder in order to clone the repo again.\n",
    "\n",
    "#@markdown **Don't delete your old work though!**\n",
    "#@markdown You will need it for this assignment.\n",
    "\n",
    "%cd $SYM_PATH\n",
    "!git clone https://github.com/LeCAR-Lab/16831-F25-HW.git\n",
    "%cd 16831-F25-HW/hw2\n",
    "%pip install -r requirements.txt\n",
    "\n",
    "# subprocess exited error at numpy is fine - install manually below\n",
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download mujoco from source (fix from hw1)\n",
    "!wget https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz\n",
    "!tar xzvf mujoco210-linux-x86_64.tar.gz\n",
    "!mkdir -p ~/.mujoco\n",
    "!mv mujoco210 ~/.mujoco/mujoco210\n",
    "!rm mujoco*\n",
    "\n",
    "%pip install -U mujoco\n",
    "%pip install -U 'mujoco-py<2.2,>=2.1'\n",
    "%pip install -U pyvirtualdisplay\n",
    "%pip install -U gym-notebook-wrapper\n",
    "%pip install -U \"cython<3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env variables\n",
    "import os\n",
    "os.environ['LD_LIBRARY_PATH'] += ':/root/.mujoco/mujoco210/bin'\n",
    "os.environ['MUJOCO_PY_MUJOCO_PATH'] = '/root/.mujoco/mujoco210'\n",
    "os.environ['LD_LIBRARY_PATH'] += ':/usr/lib/nvidia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboardx\n",
    "!pip install cement==2.10.14\n",
    "!pip install box2d box2d-py pygame --pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "noinfUbHiHW2"
   },
   "outputs": [],
   "source": [
    "#@title set up virtual display\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COqsZLeliU9Y"
   },
   "outputs": [],
   "source": [
    "#@title test virtual display\n",
    "\n",
    "#@markdown If you see a video of a four-legged ant fumbling about, setup is complete!\n",
    "import numpy as np\n",
    "if not hasattr(np, \"bool8\"):\n",
    "    np.bool8 = np.bool_\n",
    "    \n",
    "import gym\n",
    "import gnwrapper\n",
    "\n",
    "env = gnwrapper.LoopAnimation(gym.make('Ant-v4'))\n",
    "\n",
    "observation = env.reset()\n",
    "for i in range(100):\n",
    "    obs, rew, term, _ = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    if term:\n",
    "      break\n",
    "\n",
    "env.display()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygs968BbiYHr"
   },
   "source": [
    "## Editing Code\n",
    "\n",
    "To edit code, click the folder icon on the left menu. Navigate to the corresponding file (`hw_16831/...`). Double click a file to open an editor. There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window). We sync your edits to Google Drive so that you won't lose your work in the event of an instance timeout, but you will need to re-mount your Google Drive and re-install packages with every new instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qUmV93fif6S"
   },
   "source": [
    "## Run Policy Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lN-gZkqiijnR"
   },
   "outputs": [],
   "source": [
    "# fix error with autoreload\n",
    "\n",
    "#@title imports\n",
    "import importlib\n",
    "import os\n",
    "import time\n",
    "import rob831.policies.MLP_policy\n",
    "import rob831.agents.pg_agent\n",
    "import rob831.infrastructure.rl_trainer\n",
    "import rob831.infrastructure.utils\n",
    "\n",
    "importlib.reload(rob831.policies.MLP_policy)\n",
    "importlib.reload(rob831.agents.pg_agent)\n",
    "importlib.reload(rob831.infrastructure.rl_trainer)\n",
    "importlib.reload(rob831.infrastructure.utils)\n",
    "\n",
    "from rob831.infrastructure.rl_trainer import RL_Trainer\n",
    "from rob831.agents.pg_agent import PGAgent\n",
    "\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Q6NaOWhOinnU"
   },
   "outputs": [],
   "source": [
    "#@title runtime arguments\n",
    "\n",
    "class Args:\n",
    "\n",
    "  def __getitem__(self, key):\n",
    "    return getattr(self, key)\n",
    "\n",
    "  def __setitem__(self, key, val):\n",
    "    setattr(self, key, val)\n",
    "\n",
    "  def __contains__(self, key):\n",
    "    return hasattr(self, key)\n",
    "\n",
    "  env_name = 'CartPole-v0' #@param\n",
    "  exp_name = 'q1_sb_rtg_na' #@param\n",
    "\n",
    "  #@markdown main parameters of interest\n",
    "  n_iter = 100 #@param {type: \"integer\"}\n",
    "\n",
    "  ## PDF will tell you how to set ep_len\n",
    "  ## and discount for each environment\n",
    "  ep_len = 200 #@param {type: \"integer\"}\n",
    "  discount = 0.95 #@param {type: \"number\"}\n",
    "\n",
    "  reward_to_go = True #@param {type: \"boolean\"}\n",
    "  nn_baseline = False #@param {type: \"boolean\"}\n",
    "  gae_lambda = None #@param {type: \"number\"}\n",
    "  dont_standardize_advantages = False #@param {type: \"boolean\"}\n",
    "\n",
    "  #@markdown batches and steps\n",
    "  batch_size = 1000 #@param {type: \"integer\"}\n",
    "  eval_batch_size = 400 #@param {type: \"integer\"}\n",
    "\n",
    "  num_agent_train_steps_per_iter = 1 #@param {type: \"integer\"}\n",
    "  learning_rate =  5e-3 #@param {type: \"number\"}\n",
    "\n",
    "  #@markdown MLP parameters\n",
    "  n_layers = 2 #@param {type: \"integer\"}\n",
    "  size = 64 #@param {type: \"integer\"}\n",
    "\n",
    "  #@markdown system\n",
    "  save_params = False #@param {type: \"boolean\"}\n",
    "  no_gpu = False #@param {type: \"boolean\"}\n",
    "  which_gpu = 0 #@param {type: \"integer\"}\n",
    "  seed = 1 #@param {type: \"integer\"}\n",
    "\n",
    "  action_noise_std = 0 #@param {type: \"number\"}\n",
    "\n",
    "  #@markdown logging\n",
    "  ## default is to not log video so\n",
    "  ## that logs are small enough to be\n",
    "  ## uploaded to gradscope\n",
    "  video_log_freq =  -1#@param {type: \"integer\"}\n",
    "  scalar_log_freq =  1#@param {type: \"integer\"}\n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n",
    "## ensure compatibility with hw1 code\n",
    "args['train_batch_size'] = args['batch_size']\n",
    "\n",
    "if args['video_log_freq'] > 0:\n",
    "  import warnings\n",
    "  warnings.warn(\n",
    "      '''\\nLogging videos will make eventfiles too'''\n",
    "      '''\\nlarge for the autograder. Set video_log_freq = -1'''\n",
    "      '''\\nfor the runs you intend to submit.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "eScWwHhnsYkd"
   },
   "outputs": [],
   "source": [
    "#@title create directory for logging\n",
    "\n",
    "data_path = '''/content/hw_16831/hw2/data'''\n",
    "\n",
    "if not (os.path.exists(data_path)):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "logdir = args.exp_name + '_' + args.env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "logdir = os.path.join(data_path, logdir)\n",
    "args['logdir'] = logdir\n",
    "if not(os.path.exists(logdir)):\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "aljzrLdAsvNu"
   },
   "outputs": [],
   "source": [
    "## define policy gradient trainer\n",
    "\n",
    "class PG_Trainer(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "\n",
    "        #####################\n",
    "        ## SET AGENT PARAMS\n",
    "        #####################\n",
    "\n",
    "        computation_graph_args = {\n",
    "            'n_layers': params['n_layers'],\n",
    "            'size': params['size'],\n",
    "            'learning_rate': params['learning_rate'],\n",
    "            }\n",
    "\n",
    "        estimate_advantage_args = {\n",
    "            'gamma': params['discount'],\n",
    "            'standardize_advantages': not(params['dont_standardize_advantages']),\n",
    "            'reward_to_go': params['reward_to_go'],\n",
    "            'nn_baseline': params['nn_baseline'],\n",
    "            'gae_lambda': params['gae_lambda'],\n",
    "        }\n",
    "\n",
    "        train_args = {\n",
    "            'num_agent_train_steps_per_iter': params['num_agent_train_steps_per_iter'],\n",
    "        }\n",
    "\n",
    "        agent_params = {**computation_graph_args, **estimate_advantage_args, **train_args}\n",
    "\n",
    "        self.params = params\n",
    "        self.params['agent_class'] = PGAgent\n",
    "        self.params['agent_params'] = agent_params\n",
    "        self.params['batch_size_initial'] = self.params['batch_size']\n",
    "\n",
    "        ################\n",
    "        ## RL TRAINER\n",
    "        ################\n",
    "\n",
    "        self.rl_trainer = RL_Trainer(self.params)\n",
    "\n",
    "    def run_training_loop(self):\n",
    "\n",
    "        self.rl_trainer.run_training_loop(\n",
    "            self.params['n_iter'],\n",
    "            collect_policy = self.rl_trainer.agent.actor,\n",
    "            eval_policy = self.rl_trainer.agent.actor,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j2rCuQsRsd3N"
   },
   "outputs": [],
   "source": [
    "# Before running the below cell for Q3 (LunarLander), please modify lunar_lander\n",
    "\n",
    "## run training\n",
    "#@title imports\n",
    "import importlib\n",
    "import os\n",
    "import time\n",
    "import rob831.policies.MLP_policy\n",
    "import rob831.agents.pg_agent\n",
    "import rob831.infrastructure.rl_trainer\n",
    "import rob831.infrastructure.utils\n",
    "\n",
    "# path numpy for newer version\n",
    "import numpy as np\n",
    "if not hasattr(np, \"bool8\"):\n",
    "    np.bool8 = np.bool_\n",
    "\n",
    "import gym.envs.box2d.lunar_lander as lunar_lander\n",
    "\n",
    "# auto-reload lunar lander file after modifying\n",
    "importlib.reload(lunar_lander)\n",
    "\n",
    "importlib.reload(rob831.policies.MLP_policy)\n",
    "importlib.reload(rob831.agents.pg_agent)\n",
    "importlib.reload(rob831.infrastructure.rl_trainer)\n",
    "importlib.reload(rob831.infrastructure.utils)\n",
    "\n",
    "from rob831.infrastructure.rl_trainer import RL_Trainer\n",
    "from rob831.agents.pg_agent import PGAgent\n",
    "\n",
    "print(args.logdir)\n",
    "trainer = PG_Trainer(args)\n",
    "trainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exp_guide"
   },
   "source": [
    "## HW2 experiment execution guide\n",
    "Complete the setup cells above, then execute each section below in order to reproduce the results required in **hw2_new.pdf**.\n",
    "The code cells call `run_hw2.py` with the exact hyper-parameters requested by the assignment so you only need to run them sequentially in Colab.\n",
    "\n",
    "> After launches finish, use the TensorBoard cell at the bottom of the notebook to inspect learning curves and capture the plots for your write-up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cartpole_intro"
   },
   "source": [
    "### Section 5.1 \u2013 CartPole variance study\n",
    "Runs all six combinations of small/large batch size, reward-to-go, and advantage standardization.\n",
    "Collect the average-return curves from TensorBoard for both the small (`b = 1500`) and large (`b = 6000`) batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cartpole_commands"
   },
   "outputs": [],
   "source": [
    "#@title Run CartPole variance sweep (Section 5.1)\n",
    "import subprocess\n",
    "\n",
    "cartpole_commands = [\n",
    "    \"python rob831/scripts/run_hw2.py --env_name CartPole-v0 -n 150 -b 1500 -dsa --exp_name q1_sb_no_rtg_dsa\",\n",
    "    \"python rob831/scripts/run_hw2.py --env_name CartPole-v0 -n 150 -b 1500 -rtg -dsa --exp_name q1_sb_rtg_dsa\",\n",
    "    \"python rob831/scripts/run_hw2.py --env_name CartPole-v0 -n 150 -b 1500 -rtg --exp_name q1_sb_rtg_na\",\n",
    "    \"python rob831/scripts/run_hw2.py --env_name CartPole-v0 -n 150 -b 6000 -dsa --exp_name q1_lb_no_rtg_dsa\",\n",
    "    \"python rob831/scripts/run_hw2.py --env_name CartPole-v0 -n 150 -b 6000 -rtg -dsa --exp_name q1_lb_rtg_dsa\",\n",
    "    \"python rob831/scripts/run_hw2.py --env_name CartPole-v0 -n 150 -b 6000 -rtg --exp_name q1_lb_rtg_na\",\n",
    "]\n",
    "\n",
    "for command in cartpole_commands:\n",
    "    print(f\"\\n>>> Running: {command}\")\n",
    "    subprocess.run(command, shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "invpend_intro"
   },
   "source": [
    "### Section 5.2 \u2013 InvertedPendulum hyper-parameter search\n",
    "Search over batch sizes and learning rates until you find the smallest `b*` and largest `r*` that reach the optimal return of 1000 within 100 iterations.\n",
    "Record the command, iteration reaching 1000, and final return in your write-up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "invpend_search"
   },
   "outputs": [],
   "source": [
    "#@title Sweep InvertedPendulum configurations (Section 5.2)\n",
    "import subprocess\n",
    "\n",
    "invpend_batch_sizes = [1000, 2000, 4000, 8000, 16000]\n",
    "invpend_learning_rates = [0.03, 0.02, 0.015, 0.01, 0.005]\n",
    "\n",
    "for batch_size in invpend_batch_sizes:\n",
    "    for lr in invpend_learning_rates:\n",
    "        command = (\n",
    "            f\"python rob831/scripts/run_hw2.py --env_name InvertedPendulum-v4 --ep_len 1000 --discount 0.92 -n 100 -l 2 -s 64 -b {batch_size} -lr {lr} -rtg --exp_name q2_b{batch_size}_lr{lr}\"\n",
    "        )\n",
    "        print(f\"\\n>>> Running: {command}\")\n",
    "        subprocess.run(command, shell=True, check=True)\n",
    "    print(f\"Completed sweep for batch size {batch_size}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lunar_intro"
   },
   "source": [
    "### Section 7.1 \u2013 LunarLander with neural baseline\n",
    "Before running this cell, apply the PDF's modification to `lunar_lander.py`.\n",
    "The command below trains with reward-to-go and a neural network baseline; capture the learning curve and report the final return.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lunar_command"
   },
   "outputs": [],
   "source": [
    "#@title Train LunarLanderContinuous-v4 (Section 7.1)\n",
    "import subprocess\n",
    "\n",
    "lunar_command = (\n",
    "    \"python rob831/scripts/run_hw2.py --env_name LunarLanderContinuous-v4 --ep_len 1000 --discount 0.99 -n 100 -l 2 -s 64 -b 10000 -lr 0.005 --reward_to_go --nn_baseline --exp_name q3_b10000_r0.005\"\n",
    ")\n",
    "\n",
    "print(f\">>> Running: {lunar_command}\")\n",
    "subprocess.run(lunar_command, shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cheetah_intro"
   },
   "source": [
    "### Section 7.2 \u2013 HalfCheetah hyper-parameter study\n",
    "First sweep over the candidate batch sizes and learning rates, then rerun the best configuration with and without reward-to-go / neural baseline to create the ablation plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cheetah_search"
   },
   "outputs": [],
   "source": [
    "#@title Sweep HalfCheetah configurations (Section 7.2)\n",
    "import subprocess\n",
    "\n",
    "cheetah_batch_sizes = [15000, 35000, 55000]\n",
    "cheetah_learning_rates = [0.005, 0.01, 0.02]\n",
    "\n",
    "for batch_size in cheetah_batch_sizes:\n",
    "    for lr in cheetah_learning_rates:\n",
    "        command = (\n",
    "            f\"python rob831/scripts/run_hw2.py --env_name HalfCheetah-v4 --ep_len 150 --discount 0.95 -n 100 -l 2 -s 32 -b {batch_size} -lr {lr} --reward_to_go --nn_baseline --exp_name q4_search_b{batch_size}_lr{lr}\"\n",
    "        )\n",
    "        print(f\"\\n>>> Running: {command}\")\n",
    "        subprocess.run(command, shell=True, check=True)\n",
    "    print(f\"Completed sweep for batch size {batch_size}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cheetah_best"
   },
   "source": [
    "After reviewing the search runs (e.g., via TensorBoard), fill in `cheetah_b_star` and `cheetah_r_star` with the best performing configuration, then run the ablation cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cheetah_ablation"
   },
   "outputs": [],
   "source": [
    "#@title HalfCheetah ablations with the best hyper-parameters\n",
    "import subprocess\n",
    "\n",
    "cheetah_b_star = 35000  # TODO: replace with the batch size that achieved the highest final return\n",
    "cheetah_r_star = 0.01   # TODO: replace with the learning rate that achieved the highest final return\n",
    "\n",
    "cheetah_ablation_commands = [\n",
    "    f\"python rob831/scripts/run_hw2.py --env_name HalfCheetah-v4 --ep_len 150 --discount 0.95 -n 100 -l 2 -s 32 -b {cheetah_b_star} -lr {cheetah_r_star} --exp_name q4_b{cheetah_b_star}_r{cheetah_r_star}\",\n",
    "    f\"python rob831/scripts/run_hw2.py --env_name HalfCheetah-v4 --ep_len 150 --discount 0.95 -n 100 -l 2 -s 32 -b {cheetah_b_star} -lr {cheetah_r_star} -rtg --exp_name q4_b{cheetah_b_star}_r{cheetah_r_star}_rtg\",\n",
    "    f\"python rob831/scripts/run_hw2.py --env_name HalfCheetah-v4 --ep_len 150 --discount 0.95 -n 100 -l 2 -s 32 -b {cheetah_b_star} -lr {cheetah_r_star} --nn_baseline --exp_name q4_b{cheetah_b_star}_r{cheetah_r_star}_nnbaseline\",\n",
    "    f\"python rob831/scripts/run_hw2.py --env_name HalfCheetah-v4 --ep_len 150 --discount 0.95 -n 100 -l 2 -s 32 -b {cheetah_b_star} -lr {cheetah_r_star} -rtg --nn_baseline --exp_name q4_b{cheetah_b_star}_r{cheetah_r_star}_rtg_nnbaseline\",\n",
    "]\n",
    "\n",
    "for command in cheetah_ablation_commands:\n",
    "    print(f\"\\n>>> Running: {command}\")\n",
    "    subprocess.run(command, shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hopper_intro"
   },
   "source": [
    "### Section 8 \u2013 Hopper with generalized advantage estimation\n",
    "Train Hopper with reward-to-go, a neural baseline, and the four \u03bb values from the PDF.\n",
    "Summarize how \u03bb influences learning speed and final return.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hopper_runs"
   },
   "outputs": [],
   "source": [
    "#@title Run Hopper GAE sweep (Section 8)\n",
    "import subprocess\n",
    "\n",
    "hopper_lambdas = [0.0, 0.95, 0.99, 1.0]\n",
    "\n",
    "for lam in hopper_lambdas:\n",
    "    command = (\n",
    "        f\"python rob831/scripts/run_hw2.py --env_name Hopper-v4 --ep_len 1000 --discount 0.99 -n 300 -l 2 -s 32 -b 2000 -lr 0.001 -rtg --nn_baseline --gae_lambda {lam} --exp_name q5_b2000_r0.001_gae{lam}\"\n",
    "    )\n",
    "    print(f\"\\n>>> Running: {command}\")\n",
    "    subprocess.run(command, shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "execution_order"
   },
   "source": [
    "### Recommended execution order recap\n",
    "1. CartPole variance sweep (`Run CartPole variance sweep`).\n",
    "2. InvertedPendulum search (`Sweep InvertedPendulum configurations`).\n",
    "3. LunarLander baseline run (`Train LunarLanderContinuous-v4`).\n",
    "4. HalfCheetah sweep (`Sweep HalfCheetah configurations`).\n",
    "5. HalfCheetah ablations (`HalfCheetah ablations with the best hyper-parameters`).\n",
    "6. Hopper GAE sweep (`Run Hopper GAE sweep`).\n",
    "7. Launch TensorBoard (existing cell at the end of the notebook) to visualize and export plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "km7LlYvhqKTl"
   },
   "outputs": [],
   "source": [
    "#@markdown You can visualize your runs with tensorboard from within the notebook\n",
    "\n",
    "## requires tensorflow==2.3.0\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/hw_16831/hw2/data"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}