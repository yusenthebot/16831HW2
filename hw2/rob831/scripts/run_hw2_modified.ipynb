{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 16-831 HW2 \u2014 Policy Gradient Master Notebook\n",
        "\n",
        "Run every experiment from **hw2_new.pdf** by executing this notebook top to bottom in Google Colab. Each section handles the training commands, produces the requested plots, and prints the numeric analyses needed for the written deliverables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Before you start\n",
        "1. Open **File \u2192 Save a copy in Drive** so you can edit and re-run the notebook later.\n",
        "2. Execute cells sequentially with **Shift+Enter**. Avoid running expensive experiments twice unless you intend to re-train from scratch.\n",
        "3. Every section caches results in Google Drive under `hw_16831/hw2/data`. If you rerun a cell, the code will reuse the latest logs instead of retraining.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "mount_drive"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title 1\ufe0f\u20e3 Mount Google Drive\n",
        "#@markdown Your work will be saved inside `hw_16831` so Colab restarts do not wipe it.\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "set_paths"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title 2\ufe0f\u20e3 Configure homework workspace paths\n",
        "#@markdown This creates `/content/hw_16831` as a shortcut to your Drive directory and records the homework root.\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "DRIVE_WORKSPACE = Path('/content/gdrive/My Drive/hw_16831')\n",
        "DRIVE_WORKSPACE.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "COLAB_WORKSPACE = Path('/content/hw_16831')\n",
        "if COLAB_WORKSPACE.exists() and not COLAB_WORKSPACE.is_symlink():\n",
        "    raise RuntimeError('`/content/hw_16831` already exists and is not a symlink. Please rename or remove it before continuing.')\n",
        "if not COLAB_WORKSPACE.exists():\n",
        "    COLAB_WORKSPACE.symlink_to(DRIVE_WORKSPACE)\n",
        "\n",
        "HW2_ROOT = COLAB_WORKSPACE / 'hw2'\n",
        "DATA_ROOT = HW2_ROOT / 'data'\n",
        "\n",
        "os.environ['HW2_REPO_ROOT'] = str(HW2_ROOT)\n",
        "os.environ['HW2_DATA_ROOT'] = str(DATA_ROOT)\n",
        "print(f\"Homework repo directory: {HW2_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "apt_dependencies"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title 3\ufe0f\u20e3 Install system dependencies\n",
        "#@markdown Installs the packages MuJoCo and Gym rely on.\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq libosmesa6-dev libgl1-mesa-glx libglfw3 patchelf swig ffmpeg\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "clone_repo"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title 4\ufe0f\u20e3 Clone or update the homework starter code\n",
        "#@markdown Pulls the official 16-831 homework repository into your Drive workspace.\n",
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "repo_url = \"https://github.com/LeCAR-Lab/16831-S25-HW.git\"\n",
        "repo_root = Path(os.environ['HW2_REPO_ROOT'])\n",
        "if repo_root.exists() and (repo_root / '.git').exists():\n",
        "    print('Repository already present \u2014 pulling latest changes...')\n",
        "    subprocess.run(['git', 'pull'], check=True, cwd=repo_root)\n",
        "else:\n",
        "    if repo_root.exists():\n",
        "        print('Removing stale directory at', repo_root)\n",
        "        subprocess.run(['rm', '-rf', str(repo_root)], check=True)\n",
        "    subprocess.run(['git', 'clone', repo_url, str(repo_root)], check=True)\n",
        "print('Repository ready at', repo_root)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "pip_requirements"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title 5\ufe0f\u20e3 Install Python requirements\n",
        "#@markdown Installs the Python dependencies declared for HW2. Re-run after each factory reset.\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "repo_root = Path(os.environ['HW2_REPO_ROOT'])\n",
        "requirements_file = repo_root / 'requirements.txt'\n",
        "subprocess.run(['pip', 'install', '-r', str(requirements_file), '--progress-bar', 'off'], check=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "mujoco_download"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title 6\ufe0f\u20e3 Download MuJoCo 2.1.0 and configure the simulator\n",
        "#@markdown Skip this cell if you already see `~/.mujoco/mujoco210` in your Drive workspace.\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "home = Path('~').expanduser()\n",
        "mujoco_dir = home / '.mujoco'\n",
        "mujoco_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if not (mujoco_dir / 'mujoco210').exists():\n",
        "    !wget -q https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz -O /content/mujoco210-linux-x86_64.tar.gz\n",
        "    !tar -xzf /content/mujoco210-linux-x86_64.tar.gz -C {mujoco_dir}\n",
        "else:\n",
        "    print('MuJoCo 2.1.0 already present \u2014 skipping download.')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "mujoco_env"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title 7\ufe0f\u20e3 Export MuJoCo environment variables\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "home = Path('~').expanduser()\n",
        "mujoco_path = home / '.mujoco/mujoco210'\n",
        "os.environ['LD_LIBRARY_PATH'] = os.environ.get('LD_LIBRARY_PATH', '') + f\":{mujoco_path / 'bin'}\"\n",
        "os.environ['MUJOCO_PY_MUJOCO_PATH'] = str(mujoco_path)\n",
        "os.environ['MUJOCO_GL'] = 'egl'\n",
        "print('LD_LIBRARY_PATH ->', os.environ['LD_LIBRARY_PATH'])\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "extra_packages"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title 8\ufe0f\u20e3 Install reinforcement-learning helper packages\n",
        "#@markdown These packages support logging, Box2D environments, and MuJoCo rendering.\n",
        "!pip install -q tensorboardX box2d box2d-py pygame==2.1.3\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "virtual_display"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title 9\ufe0f\u20e3 Start a virtual display (required for MuJoCo)\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "print('Virtual display started.')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "render_check"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title \ud83d\udd1f Sanity check rendering setup\n",
        "#@markdown You should see a short video of a MuJoCo Ant if everything is configured correctly.\n",
        "import imageio\n",
        "import numpy as np\n",
        "import gym\n",
        "from IPython.display import Image\n",
        "\n",
        "env = gym.make('Ant-v4')\n",
        "obs, _ = env.reset(seed=0, return_info=True)\n",
        "frames = []\n",
        "for _ in range(30):\n",
        "    obs, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
        "    frame = env.render()\n",
        "    frames.append(frame)\n",
        "    if terminated or truncated:\n",
        "        break\n",
        "env.close()\n",
        "imageio.mimsave('/content/ant_preview.gif', frames, fps=10)\n",
        "Image(filename='/content/ant_preview.gif')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utilities for running experiments and aggregating results\n",
        "The helpers below take care of launching `run_hw2.py`, collecting TensorBoard scalars, and formatting the plots required in the PDF. They also remember which commands you already ran so repeated executions reuse cached results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "helper_functions"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Helper functions for experiment orchestration\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import shlex\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import Markdown, display\n",
        "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "pd.set_option('display.max_rows', 200)\n",
        "pd.set_option('display.precision', 2)\n",
        "\n",
        "PYTHON_BIN = sys.executable\n",
        "REPO_ROOT = Path(os.environ['HW2_REPO_ROOT'])\n",
        "DATA_ROOT = Path(os.environ.get('HW2_DATA_ROOT', REPO_ROOT / 'data'))\n",
        "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RUN_REGISTRY = {}\n",
        "\n",
        "\n",
        "def _normalize_command(command: str):\n",
        "    tokens = shlex.split(command)\n",
        "    if not tokens:\n",
        "        raise ValueError('Empty command string provided.')\n",
        "    if tokens[0] in {'python', 'python3'} or tokens[0].endswith('python3') or tokens[0].endswith('python'):\n",
        "        tokens[0] = PYTHON_BIN\n",
        "    else:\n",
        "        tokens = [PYTHON_BIN] + tokens\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def _detect_exp_name(tokens, explicit=None):\n",
        "    if explicit is not None:\n",
        "        return explicit\n",
        "    for idx, token in enumerate(tokens):\n",
        "        if token == '--exp_name' and idx + 1 < len(tokens):\n",
        "            return tokens[idx + 1]\n",
        "    raise ValueError('Every command must include --exp_name to disambiguate log directories.')\n",
        "\n",
        "\n",
        "def list_logdirs(exp_name: str):\n",
        "    pattern = f\"{exp_name}_\"\n",
        "    if not DATA_ROOT.exists():\n",
        "        return []\n",
        "    return sorted([\n",
        "        path for path in DATA_ROOT.iterdir()\n",
        "        if path.is_dir() and path.name.startswith(pattern)\n",
        "    ], key=lambda p: p.stat().st_mtime)\n",
        "\n",
        "\n",
        "def latest_logdir(exp_name: str):\n",
        "    matches = list_logdirs(exp_name)\n",
        "    return matches[-1] if matches else None\n",
        "\n",
        "\n",
        "def run_pg_command(command: str, exp_name: str = None, force: bool = False):\n",
        "    tokens = _normalize_command(command)\n",
        "    exp_name = _detect_exp_name(tokens, explicit=exp_name)\n",
        "    existing = latest_logdir(exp_name)\n",
        "    if existing is not None and not force:\n",
        "        print(f\"Skipping {exp_name}: reusing cached logs at {existing.name}\")\n",
        "        RUN_REGISTRY.setdefault(exp_name, {})['logdir'] = str(existing)\n",
        "        RUN_REGISTRY[exp_name]['command'] = command\n",
        "        return existing\n",
        "    print(f\"Launching {exp_name}...\")\n",
        "    subprocess.run(tokens, cwd=REPO_ROOT, check=True)\n",
        "    logdir = latest_logdir(exp_name)\n",
        "    if logdir is None:\n",
        "        raise FileNotFoundError(f\"Failed to locate log directory for {exp_name}.\")\n",
        "    RUN_REGISTRY[exp_name] = {'command': command, 'logdir': str(logdir)}\n",
        "    print(f\"Finished {exp_name}, logs saved to {logdir.name}\")\n",
        "    return logdir\n",
        "\n",
        "\n",
        "def run_experiment_batch(run_configs, force: bool = False):\n",
        "    completed = []\n",
        "    for cfg in run_configs:\n",
        "        logdir = run_pg_command(cfg['command'], exp_name=cfg['exp_name'], force=force)\n",
        "        cfg_record = dict(cfg)\n",
        "        cfg_record['logdir'] = str(logdir)\n",
        "        completed.append(cfg_record)\n",
        "    return completed\n",
        "\n",
        "\n",
        "def load_scalar_curve(exp_name: str, tag: str):\n",
        "    logdir = latest_logdir(exp_name)\n",
        "    if logdir is None:\n",
        "        raise FileNotFoundError(f\"No log directory found for {exp_name}\")\n",
        "    accumulator = EventAccumulator(str(logdir), size_guidance={'scalars': 0})\n",
        "    accumulator.Reload()\n",
        "    scalar_tags = accumulator.Tags().get('scalars', [])\n",
        "    if tag not in scalar_tags:\n",
        "        json_path = logdir / 'scalar_data.json'\n",
        "        if json_path.exists():\n",
        "            raw = json.loads(json_path.read_text())\n",
        "            series = raw.get(tag, [])\n",
        "            steps = np.array([entry['step'] for entry in series], dtype=np.int64)\n",
        "            values = np.array([entry['value'] for entry in series], dtype=np.float64)\n",
        "            return steps, values, logdir\n",
        "        raise KeyError(f\"Tag {tag} not logged for experiment {exp_name}\")\n",
        "    events = accumulator.Scalars(tag)\n",
        "    steps = np.array([event.step for event in events], dtype=np.int64)\n",
        "    values = np.array([event.value for event in events], dtype=np.float64)\n",
        "    return steps, values, logdir\n",
        "\n",
        "\n",
        "def compile_curves(label_exp_pairs, metric: str = 'Eval_AverageReturn'):\n",
        "    frames = []\n",
        "    for label, exp_name in label_exp_pairs:\n",
        "        steps, values, logdir = load_scalar_curve(exp_name, metric)\n",
        "        try:\n",
        "            env_steps, env_vals, _ = load_scalar_curve(exp_name, 'Train_EnvstepsSoFar')\n",
        "            env_map = dict(zip(env_steps.tolist(), env_vals.tolist()))\n",
        "        except Exception:\n",
        "            env_map = {}\n",
        "        frame = pd.DataFrame({\n",
        "            'iteration': steps,\n",
        "            'value': values,\n",
        "            'envsteps': [env_map.get(int(step), np.nan) for step in steps],\n",
        "            'label': label,\n",
        "            'exp_name': exp_name,\n",
        "            'logdir': str(logdir),\n",
        "        })\n",
        "        frames.append(frame)\n",
        "    if not frames:\n",
        "        return pd.DataFrame(columns=['iteration', 'value', 'envsteps', 'label', 'exp_name', 'logdir'])\n",
        "    return pd.concat(frames, ignore_index=True)\n",
        "\n",
        "\n",
        "def plot_learning_curves(df, title: str, ylabel: str = 'Eval Average Return', xlabel: str = 'Training iteration', target: float = None):\n",
        "    if df.empty:\n",
        "        raise ValueError('No scalar data available to plot.')\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    for label, group in df.groupby('label'):\n",
        "        ordered = group.sort_values('iteration')\n",
        "        plt.plot(ordered['iteration'], ordered['value'], label=label, linewidth=2)\n",
        "    if target is not None:\n",
        "        plt.axhline(target, linestyle='--', linewidth=1.5, color='tab:gray', label=f'Target = {target}')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', linewidth=0.6, alpha=0.4)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return df\n",
        "\n",
        "\n",
        "def summarize_experiments(label_exp_pairs, metric: str = 'Eval_AverageReturn', threshold: float = None):\n",
        "    rows = []\n",
        "    for label, exp_name in label_exp_pairs:\n",
        "        steps, values, logdir = load_scalar_curve(exp_name, metric)\n",
        "        record = {\n",
        "            'label': label,\n",
        "            'exp_name': exp_name,\n",
        "            'logdir': str(logdir),\n",
        "            'final_iteration': int(steps[-1]),\n",
        "            'final_return': float(values[-1]),\n",
        "            'best_return': float(values.max()),\n",
        "            'command': RUN_REGISTRY.get(exp_name, {}).get('command', '')\n",
        "        }\n",
        "        if threshold is not None:\n",
        "            hits = np.where(values >= threshold)[0]\n",
        "            record['iteration_reaching_threshold'] = int(steps[hits[0]]) if hits.size else None\n",
        "        rows.append(record)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def describe_threshold(summary_df, column='iteration_reaching_threshold'):\n",
        "    valid = summary_df.dropna(subset=[column])\n",
        "    if valid.empty:\n",
        "        return 'None of the runs reached the requested threshold.'\n",
        "    best_row = valid.sort_values(column).iloc[0]\n",
        "    return f\"Fastest run: {best_row['label']} reached the threshold at iteration {int(best_row[column])}.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 1 \u2013 CartPole variance study (Section 5.1)\n",
        "Run the six CartPole configurations exactly as listed in the PDF. The two plots compare (a) the three small-batch runs and (b) the three large-batch runs. The analysis cell automatically answers the written questions about value estimators, advantage standardization, and batch size.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "cartpole_run"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "cartpole_runs = [\n",
        "    {\n",
        "        'label': 'b=1500, trajectory returns (no RTG, no std)',\n",
        "        'group': 'small',\n",
        "        'exp_name': 'q1_sb_no_rtg_dsa',\n",
        "        'command': 'python rob831/scripts/run_hw2.py --env_name CartPole-v0 -n 150 -b 1500 -dsa --exp_name q1_sb_no_rtg_dsa'\n",
        "    },\n",
        "    {\n",
        "        'label': 'b=1500, reward-to-go (no std)',\n",
        "        'group': 'small',\n",
        "        'exp_name': 'q1_sb_rtg_dsa',\n",
        "        'command': 'python rob831/scripts/run_hw2.py --env_name CartPole-v0 -n 150 -b 1500 -rtg -dsa --exp_name q1_sb_rtg_dsa'\n",
        "    },\n",
        "    {\n",
        "        'label': 'b=1500, reward-to-go + std advantages',\n",
        "        'group': 'small',\n",
        "        'exp_name': 'q1_sb_rtg_na',\n",
        "        'command': 'python rob831/scripts/run_hw2.py --env_name CartPole-v0 -n 150 -b 1500 -rtg --exp_name q1_sb_rtg_na'\n",
        "    },\n",
        "    {\n",
        "        'label': 'b=6000, trajectory returns (no RTG, no std)',\n",
        "        'group': 'large',\n",
        "        'exp_name': 'q1_lb_no_rtg_dsa',\n",
        "        'command': 'python rob831/scripts/run_hw2.py --env_name CartPole-v0 -n 150 -b 6000 -dsa --exp_name q1_lb_no_rtg_dsa'\n",
        "    },\n",
        "    {\n",
        "        'label': 'b=6000, reward-to-go (no std)',\n",
        "        'group': 'large',\n",
        "        'exp_name': 'q1_lb_rtg_dsa',\n",
        "        'command': 'python rob831/scripts/run_hw2.py --env_name CartPole-v0 -n 150 -b 6000 -rtg -dsa --exp_name q1_lb_rtg_dsa'\n",
        "    },\n",
        "    {\n",
        "        'label': 'b=6000, reward-to-go + std advantages',\n",
        "        'group': 'large',\n",
        "        'exp_name': 'q1_lb_rtg_na',\n",
        "        'command': 'python rob831/scripts/run_hw2.py --env_name CartPole-v0 -n 150 -b 6000 -rtg --exp_name q1_lb_rtg_na'\n",
        "    },\n",
        "]\n",
        "\n",
        "cartpole_runs = run_experiment_batch(cartpole_runs)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "cartpole_plots"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "small_entries = [(cfg['label'], cfg['exp_name']) for cfg in cartpole_runs if cfg['group'] == 'small']\n",
        "large_entries = [(cfg['label'], cfg['exp_name']) for cfg in cartpole_runs if cfg['group'] == 'large']\n",
        "\n",
        "cartpole_small_df = compile_curves(small_entries)\n",
        "plot_learning_curves(cartpole_small_df, 'CartPole-v0 (batch size 1500)', target=200)\n",
        "\n",
        "cartpole_large_df = compile_curves(large_entries)\n",
        "plot_learning_curves(cartpole_large_df, 'CartPole-v0 (batch size 6000)', target=200)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "cartpole_analysis"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "cartpole_small_summary = summarize_experiments(small_entries)\n",
        "cartpole_large_summary = summarize_experiments(large_entries)\n",
        "\n",
        "display(Markdown('**Small batch summary (b = 1500)**'))\n",
        "display(cartpole_small_summary)\n",
        "\n",
        "display(Markdown('**Large batch summary (b = 6000)**'))\n",
        "display(cartpole_large_summary)\n",
        "\n",
        "traj_small = cartpole_small_summary.loc[cartpole_small_summary['exp_name'] == 'q1_sb_no_rtg_dsa', 'final_return'].item()\n",
        "rtg_small = cartpole_small_summary.loc[cartpole_small_summary['exp_name'] == 'q1_sb_rtg_dsa', 'final_return'].item()\n",
        "std_small = cartpole_small_summary.loc[cartpole_small_summary['exp_name'] == 'q1_sb_rtg_na', 'final_return'].item()\n",
        "\n",
        "traj_large = cartpole_large_summary.loc[cartpole_large_summary['exp_name'] == 'q1_lb_no_rtg_dsa', 'final_return'].item()\n",
        "rtg_large = cartpole_large_summary.loc[cartpole_large_summary['exp_name'] == 'q1_lb_rtg_dsa', 'final_return'].item()\n",
        "std_large = cartpole_large_summary.loc[cartpole_large_summary['exp_name'] == 'q1_lb_rtg_na', 'final_return'].item()\n",
        "\n",
        "small_best = cartpole_small_summary['final_return'].max()\n",
        "large_best = cartpole_large_summary['final_return'].max()\n",
        "\n",
        "analysis_lines = [\n",
        "    f\"- **Value estimator:** Without standardization (b=1500), reward-to-go finishes at {rtg_small:.1f} vs {traj_small:.1f} for trajectory returns, so reward-to-go performs better.\",\n",
        "    f\"- **Advantage standardization:** At b=1500 the final return rises from {rtg_small:.1f} to {std_small:.1f} when advantages are normalized; at b=6000 the boost is {rtg_large:.1f} \u2192 {std_large:.1f}.\",\n",
        "    f\"- **Batch size:** The best small-batch run ends at {small_best:.1f} average return, whereas the best large-batch run reaches {large_best:.1f}, showing that larger batches deliver higher asymptotic performance on CartPole.\"\n",
        "]\n",
        "\n",
        "display(Markdown('### Written answers for Section 5.1\n",
        "' + '\n",
        "'.join(analysis_lines)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 2 \u2013 InvertedPendulum hyper-parameter search (Section 5.2)\n",
        "We search for the smallest batch size `b*` and the largest learning rate `r*` that reach the optimal return of 1000 in fewer than 100 iterations. The cell below sweeps over the provided candidates until a qualifying configuration is found, then the subsequent cells plot the successful run and record the exact command.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "invpend_search"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "invpend_batch_sizes = [1000, 2000, 4000, 8000, 16000]\n",
        "invpend_learning_rates = [0.03, 0.02, 0.015, 0.01, 0.005]\n",
        "\n",
        "invpend_attempts = []\n",
        "best_invpend = None\n",
        "\n",
        "for batch_size in invpend_batch_sizes:\n",
        "    for lr in invpend_learning_rates:\n",
        "        exp_name = f\"q2_b{batch_size}_lr{lr}\"\n",
        "        label = f\"b={batch_size}, lr={lr}\"\n",
        "        command = (\n",
        "            f\"python rob831/scripts/run_hw2.py --env_name InvertedPendulum-v4 \"\n",
        "            f\"--ep_len 1000 --discount 0.92 -n 100 -l 2 -s 64 -b {batch_size} -lr {lr} \"\n",
        "            f\"-rtg --exp_name {exp_name}\"\n",
        "        )\n",
        "        run_pg_command(command, exp_name=exp_name)\n",
        "        summary = summarize_experiments([(label, exp_name)], threshold=1000.0)\n",
        "        record = summary.iloc[0].to_dict()\n",
        "        record['batch_size'] = batch_size\n",
        "        record['learning_rate'] = lr\n",
        "        invpend_attempts.append(record)\n",
        "        if record['best_return'] >= 1000 and best_invpend is None:\n",
        "            best_invpend = record\n",
        "            break\n",
        "    if best_invpend is not None:\n",
        "        break\n",
        "\n",
        "invpend_attempts_df = pd.DataFrame(invpend_attempts)\n",
        "display(Markdown('**All attempted configurations (ordered search)**'))\n",
        "display(invpend_attempts_df)\n",
        "\n",
        "if best_invpend is None:\n",
        "    raise RuntimeError('Search did not find a configuration that reaches the optimal return. Expand the candidate sets and rerun this cell.')\n",
        "else:\n",
        "    display(Markdown(\n",
        "        f\"\u2705 Found b* = {best_invpend['batch_size']} and r* = {best_invpend['learning_rate']} reaching 1000 in \"\n",
        "        f\"iteration {int(best_invpend.get('iteration_reaching_threshold', 0))}.\"\n",
        "    ))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "invpend_deliverable"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "best_invpend_label = f\"b={best_invpend['batch_size']}, lr={best_invpend['learning_rate']}\"\n",
        "best_invpend_exp = f\"q2_b{best_invpend['batch_size']}_lr{best_invpend['learning_rate']}\"\n",
        "\n",
        "invpend_curve = compile_curves([(best_invpend_label, best_invpend_exp)])\n",
        "plot_learning_curves(invpend_curve, 'InvertedPendulum-v4 best configuration', target=1000)\n",
        "\n",
        "best_summary = summarize_experiments([(best_invpend_label, best_invpend_exp)], threshold=1000.0)\n",
        "best_command = best_summary.loc[0, 'command']\n",
        "\n",
        "summary_lines = [\n",
        "    f\"- Command: `{best_command}`\",\n",
        "    f\"- Iteration reaching 1000: {int(best_summary.loc[0, 'iteration_reaching_threshold'])}\",\n",
        "    f\"- Final average return: {best_summary.loc[0, 'final_return']:.1f}\",\n",
        "]\n",
        "\n",
        "display(Markdown('### Deliverables for Section 5.2\n",
        "' + '\n",
        "'.join(summary_lines)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 3 \u2013 LunarLander continuous control (Section 7.1)\n",
        "This section validates the policy-gradient implementation with neural baseline on a moderate-difficulty task. Make sure you have applied the PDF\u2019s code edits to `lunar_lander.py` before running the command below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "lunar_command"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "lunar_run = [\n",
        "    {\n",
        "        'label': 'LunarLanderContinuous-v4 baseline',\n",
        "        'exp_name': 'q3_b10000_r0.005',\n",
        "        'command': 'python rob831/scripts/run_hw2.py --env_name LunarLanderContinuous-v4 --ep_len 1000 --discount 0.99 -n 100 -l 2 -s 64 -b 10000 -lr 0.005 --reward_to_go --nn_baseline --exp_name q3_b10000_r0.005'\n",
        "    }\n",
        "]\n",
        "\n",
        "lunar_run = run_experiment_batch(lunar_run)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "lunar_plot"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "lunar_entries = [(cfg['label'], cfg['exp_name']) for cfg in lunar_run]\n",
        "lunar_curve = compile_curves(lunar_entries)\n",
        "plot_learning_curves(lunar_curve, 'LunarLanderContinuous-v4 policy gradient', target=120)\n",
        "\n",
        "lunar_summary = summarize_experiments(lunar_entries)\n",
        "display(Markdown('**Section 7.1 deliverable:**'))\n",
        "display(lunar_summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 4 \u2013 HalfCheetah hyper-parameter study (Section 7.2)\n",
        "We first sweep over the prescribed batch sizes and learning rates using reward-to-go with a neural baseline to identify \\(b^*, r^*\\). Afterwards we run the four ablations (no baseline/no RTG, RTG only, baseline only, both) with those hyper-parameters, produce the requested plots, and summarize how \\(b\\) and \\(r\\) affect performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "cheetah_search"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "cheetah_batch_sizes = [15000, 35000, 55000]\n",
        "cheetah_learning_rates = [0.005, 0.01, 0.02]\n",
        "\n",
        "cheetah_search_configs = []\n",
        "for batch_size in cheetah_batch_sizes:\n",
        "    for lr in cheetah_learning_rates:\n",
        "        exp_name = f\"q4_search_b{batch_size}_lr{lr}\"\n",
        "        label = f\"b={batch_size}, lr={lr}\"\n",
        "        command = (\n",
        "            f\"python rob831/scripts/run_hw2.py --env_name HalfCheetah-v4 --ep_len 150 \"\n",
        "            f\"--discount 0.95 -n 100 -l 2 -s 32 -b {batch_size} -lr {lr} -rtg --nn_baseline \"\n",
        "            f\"--exp_name {exp_name}\"\n",
        "        )\n",
        "        cheetah_search_configs.append({'label': label, 'exp_name': exp_name, 'command': command, 'batch_size': batch_size, 'learning_rate': lr})\n",
        "\n",
        "cheetah_search_runs = run_experiment_batch(cheetah_search_configs)\n",
        "\n",
        "search_entries = [(cfg['label'], cfg['exp_name']) for cfg in cheetah_search_runs]\n",
        "cheetah_search_summary = summarize_experiments(search_entries)\n",
        "\n",
        "cheetah_search_summary['batch_size'] = [cfg['batch_size'] for cfg in cheetah_search_runs]\n",
        "cheetah_search_summary['learning_rate'] = [cfg['learning_rate'] for cfg in cheetah_search_runs]\n",
        "\n",
        "display(Markdown('**Hyper-parameter sweep results**'))\n",
        "display(cheetah_search_summary.sort_values('final_return', ascending=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "cheetah_search_plot"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "cheetah_search_df = compile_curves(search_entries)\n",
        "plot_learning_curves(cheetah_search_df, 'HalfCheetah-v4 search runs (RTG + baseline)', target=200)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "cheetah_final_run"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "cheetah_best_row = cheetah_search_summary.sort_values('final_return', ascending=False).iloc[0]\n",
        "cheetah_b_star = int(cheetah_best_row['batch_size'])\n",
        "cheetah_r_star = cheetah_best_row['learning_rate']\n",
        "print(f\"Chosen b* = {cheetah_b_star}, r* = {cheetah_r_star}\")\n",
        "\n",
        "cheetah_final_runs = [\n",
        "    {\n",
        "        'label': 'No RTG, no baseline',\n",
        "        'exp_name': f'q4_b{cheetah_b_star}_r{cheetah_r_star}',\n",
        "        'command': f'python rob831/scripts/run_hw2.py --env_name HalfCheetah-v4 --ep_len 150 --discount 0.95 -n 100 -l 2 -s 32 -b {cheetah_b_star} -lr {cheetah_r_star} --exp_name q4_b{cheetah_b_star}_r{cheetah_r_star}'\n",
        "    },\n",
        "    {\n",
        "        'label': 'Reward-to-go only',\n",
        "        'exp_name': f'q4_b{cheetah_b_star}_r{cheetah_r_star}_rtg',\n",
        "        'command': f'python rob831/scripts/run_hw2.py --env_name HalfCheetah-v4 --ep_len 150 --discount 0.95 -n 100 -l 2 -s 32 -b {cheetah_b_star} -lr {cheetah_r_star} -rtg --exp_name q4_b{cheetah_b_star}_r{cheetah_r_star}_rtg'\n",
        "    },\n",
        "    {\n",
        "        'label': 'NN baseline only',\n",
        "        'exp_name': f'q4_b{cheetah_b_star}_r{cheetah_r_star}_nnbaseline',\n",
        "        'command': f'python rob831/scripts/run_hw2.py --env_name HalfCheetah-v4 --ep_len 150 --discount 0.95 -n 100 -l 2 -s 32 -b {cheetah_b_star} -lr {cheetah_r_star} --nn_baseline --exp_name q4_b{cheetah_b_star}_r{cheetah_r_star}_nnbaseline'\n",
        "    },\n",
        "    {\n",
        "        'label': 'Reward-to-go + baseline',\n",
        "        'exp_name': f'q4_b{cheetah_b_star}_r{cheetah_r_star}_rtg_nnbaseline',\n",
        "        'command': f'python rob831/scripts/run_hw2.py --env_name HalfCheetah-v4 --ep_len 150 --discount 0.95 -n 100 -l 2 -s 32 -b {cheetah_b_star} -lr {cheetah_r_star} -rtg --nn_baseline --exp_name q4_b{cheetah_b_star}_r{cheetah_r_star}_rtg_nnbaseline'\n",
        "    }\n",
        "]\n",
        "\n",
        "cheetah_final_runs = run_experiment_batch(cheetah_final_runs)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "cheetah_final_summary"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "cheetah_final_entries = [(cfg['label'], cfg['exp_name']) for cfg in cheetah_final_runs]\n",
        "cheetah_final_df = compile_curves(cheetah_final_entries)\n",
        "plot_learning_curves(cheetah_final_df, f'HalfCheetah-v4 ablations (b={cheetah_b_star}, lr={cheetah_r_star})', target=200)\n",
        "\n",
        "cheetah_final_summary = summarize_experiments(cheetah_final_entries)\n",
        "display(Markdown('**Section 7.2 ablation summary**'))\n",
        "display(cheetah_final_summary)\n",
        "\n",
        "batch_effects = cheetah_search_summary.groupby('batch_size')['final_return'].mean().sort_index()\n",
        "lr_effects = cheetah_search_summary.groupby('learning_rate')['final_return'].mean().sort_index()\n",
        "\n",
        "analysis_text = [\n",
        "    '- **Batch size effect:** ' + ', '.join([f\"b={int(bs)} \u2192 avg return {val:.1f}\" for bs, val in batch_effects.items()]),\n",
        "    '- **Learning rate effect:** ' + ', '.join([f\"lr={lr} \u2192 avg return {val:.1f}\" for lr, val in lr_effects.items()]),\n",
        "    f\"- **Best configuration:** b* = {cheetah_b_star}, r* = {cheetah_r_star} with final return {cheetah_best_row['final_return']:.1f}.\"\n",
        "]\n",
        "\n",
        "display(Markdown('### Written discussion for Section 7.2\n",
        "' + '\n",
        "'.join(analysis_text)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 5 \u2013 Hopper with generalized advantage estimation (Section 8)\n",
        "Run the noisy Hopper environment with reward-to-go, a neural baseline, and \\(\\lambda \\in \\{0, 0.95, 0.99, 1.0\\}\\). The plots compare the four curves and the analysis cell highlights how \\(\\lambda\\) influences learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "hopper_runs"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "hopper_lambdas = [0.0, 0.95, 0.99, 1.0]\n",
        "\n",
        "hopper_runs = []\n",
        "for lam in hopper_lambdas:\n",
        "    lambda_tag = str(lam).replace('.', 'p')\n",
        "    exp_name = f\"q5_b2000_r0.001_lambda{lambda_tag}\"\n",
        "    label = f\"lambda={lam}\"\n",
        "    command = (\n",
        "        f\"python rob831/scripts/run_hw2.py --env_name Hopper-v4 --ep_len 1000 --discount 0.99 -n 300 -l 2 -s 32 -b 2000 -lr 0.001 \"\n",
        "        f\"--reward_to_go --nn_baseline --action_noise_std 0.5 --gae_lambda {lam} --exp_name {exp_name}\"\n",
        "    )\n",
        "    hopper_runs.append({'label': label, 'exp_name': exp_name, 'command': command, 'lambda': lam})\n",
        "\n",
        "hopper_runs = run_experiment_batch(hopper_runs)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "hopper_analysis"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "hopper_entries = [(cfg['label'], cfg['exp_name']) for cfg in hopper_runs]\n",
        "hopper_df = compile_curves(hopper_entries)\n",
        "plot_learning_curves(hopper_df, 'Hopper-v4 with GAE sweep', target=400)\n",
        "\n",
        "hopper_summary = summarize_experiments(hopper_entries)\n",
        "hopper_summary['lambda'] = hopper_lambdas\n",
        "\n",
        "display(Markdown('**Section 8 summary**'))\n",
        "display(hopper_summary)\n",
        "\n",
        "best_idx = hopper_summary['final_return'].idxmax()\n",
        "best_lambda = hopper_summary.loc[best_idx, 'lambda']\n",
        "best_return = hopper_summary.loc[best_idx, 'final_return']\n",
        "\n",
        "discussion = [\n",
        "    '- Detailed returns: ' + ', '.join([f\"lambda={row['lambda']} \u2192 final {row['final_return']:.1f}\" for _, row in hopper_summary.iterrows()]),\n",
        "    f\"- The best performing setting is lambda={best_lambda} with final return {best_return:.1f}, showing how increasing lambda changes bias/variance.\" ,\n",
        "    f\"- Command for best run: `{hopper_summary.loc[best_idx, 'command']}`\"\n",
        "]\n",
        "\n",
        "display(Markdown('### Written discussion for Section 8\n",
        "' + '\n",
        "'.join(discussion)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Appendix \u2013 Full experiment execution order\n",
        "1. **CartPole variance study** (Cell `cartpole_run` \u2192 plots \u2192 analysis).\n",
        "2. **InvertedPendulum search** (Cells `invpend_search` and `invpend_deliverable`).\n",
        "3. **LunarLander baseline** (Cells `lunar_command` and `lunar_plot`).\n",
        "4. **HalfCheetah sweep and ablations** (Cells `cheetah_search` \u2192 `cheetah_final_summary`).\n",
        "5. **Hopper with GAE** (Cells `hopper_runs` and `hopper_analysis`).\n",
        "\n",
        "Once these cells have been executed in sequence, the notebook contains every plot, table, and written answer requested in *hw2_new.pdf* and is ready for export.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}