{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gwo9bpaVgxXF"
   },
   "source": [
    "##Setup\n",
    "\n",
    "You will need to make a copy of this notebook in your Google Drive before you can edit the homework files. You can do so with **File &rarr; Save a copy in Drive**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6CAdiyTKi4Se"
   },
   "outputs": [],
   "source": [
    "#@title mount your Google Drive\n",
    "#@markdown Your work will be stored in a folder called `hw_16831` by default to prevent Colab instance timeouts from deleting your edits.\n",
    "\n",
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BKE5nA1Fgwwy"
   },
   "outputs": [],
   "source": [
    "#@title set up mount symlink\n",
    "\n",
    "DRIVE_PATH = '/content/gdrive/My\\ Drive/hw_16831'\n",
    "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
    "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
    "  %mkdir $DRIVE_PATH\n",
    "\n",
    "## the space in `My Drive` causes some issues,\n",
    "## make a symlink to avoid this\n",
    "SYM_PATH = '/content/hw_16831'\n",
    "if not os.path.exists(SYM_PATH):\n",
    "  !ln -s $DRIVE_PATH $SYM_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FGK4kbpg3iP"
   },
   "outputs": [],
   "source": [
    "#@title apt install requirements\n",
    "\n",
    "#@markdown Run each section with Shift+Enter\n",
    "\n",
    "#@markdown Double-click on section headers to show code.\n",
    "\n",
    "!apt update\n",
    "!apt install -y --no-install-recommends \\\n",
    "        build-essential \\\n",
    "        curl \\\n",
    "        git \\\n",
    "        gnupg2 \\\n",
    "        make \\\n",
    "        cmake \\\n",
    "        ffmpeg \\\n",
    "        swig \\\n",
    "        libz-dev \\\n",
    "        unzip \\\n",
    "        zlib1g-dev \\\n",
    "        libglfw3 \\\n",
    "        libglfw3-dev \\\n",
    "        libxrandr2 \\\n",
    "        libxinerama-dev \\\n",
    "        libxi6 \\\n",
    "        libxcursor-dev \\\n",
    "        libgl1-mesa-dev \\\n",
    "        libgl1-mesa-glx \\\n",
    "        libglew-dev \\\n",
    "        libosmesa6-dev \\\n",
    "        lsb-release \\\n",
    "        ack-grep \\\n",
    "        patchelf \\\n",
    "        wget \\\n",
    "        xpra \\\n",
    "        xserver-xorg-dev \\\n",
    "        xvfb \\\n",
    "        python3-opengl \\\n",
    "        ffmpeg > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftV6HiHza3E-"
   },
   "outputs": [],
   "source": [
    "#@title clone homework repo\n",
    "#@markdown Note that this is the same codebase from homework 1,\n",
    "#@markdown so you may need to move your old `hw_16831`\n",
    "#@markdown folder in order to clone the repo again.\n",
    "\n",
    "#@markdown **Don't delete your old work though!**\n",
    "#@markdown You will need it for this assignment.\n",
    "\n",
    "%cd $SYM_PATH\n",
    "!git clone https://github.com/LeCAR-Lab/16831-F25-HW.git\n",
    "%cd 16831-F25-HW/hw2\n",
    "%pip install -r requirements.txt\n",
    "\n",
    "# subprocess exited error at numpy is fine - install manually below\n",
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download mujoco from source (fix from hw1)\n",
    "!wget https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz\n",
    "!tar xzvf mujoco210-linux-x86_64.tar.gz\n",
    "!mkdir -p ~/.mujoco\n",
    "!mv mujoco210 ~/.mujoco/mujoco210\n",
    "!rm mujoco*\n",
    "\n",
    "%pip install -U mujoco\n",
    "%pip install -U 'mujoco-py<2.2,>=2.1'\n",
    "%pip install -U pyvirtualdisplay\n",
    "%pip install -U gym-notebook-wrapper\n",
    "%pip install -U \"cython<3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env variables\n",
    "import os\n",
    "os.environ['LD_LIBRARY_PATH'] += ':/root/.mujoco/mujoco210/bin'\n",
    "os.environ['MUJOCO_PY_MUJOCO_PATH'] = '/root/.mujoco/mujoco210'\n",
    "os.environ['LD_LIBRARY_PATH'] += ':/usr/lib/nvidia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboardx\n",
    "!pip install cement==2.10.14\n",
    "!pip install box2d box2d-py pygame --pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "noinfUbHiHW2"
   },
   "outputs": [],
   "source": [
    "#@title set up virtual display\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COqsZLeliU9Y"
   },
   "outputs": [],
   "source": [
    "#@title test virtual display\n",
    "\n",
    "#@markdown If you see a video of a four-legged ant fumbling about, setup is complete!\n",
    "import numpy as np\n",
    "if not hasattr(np, \"bool8\"):\n",
    "    np.bool8 = np.bool_\n",
    "    \n",
    "import gym\n",
    "import gnwrapper\n",
    "\n",
    "env = gnwrapper.LoopAnimation(gym.make('Ant-v4'))\n",
    "\n",
    "observation = env.reset()\n",
    "for i in range(100):\n",
    "    obs, rew, term, _ = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    if term:\n",
    "      break\n",
    "\n",
    "env.display()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygs968BbiYHr"
   },
   "source": [
    "## Editing Code\n",
    "\n",
    "To edit code, click the folder icon on the left menu. Navigate to the corresponding file (`hw_16831/...`). Double click a file to open an editor. There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window). We sync your edits to Google Drive so that you won't lose your work in the event of an instance timeout, but you will need to re-mount your Google Drive and re-install packages with every new instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qUmV93fif6S"
   },
   "source": [
    "## Run Policy Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lN-gZkqiijnR"
   },
   "outputs": [],
   "source": [
    "# fix error with autoreload\n",
    "\n",
    "#@title imports\n",
    "import importlib\n",
    "import os\n",
    "import time\n",
    "import rob831.policies.MLP_policy\n",
    "import rob831.agents.pg_agent\n",
    "import rob831.infrastructure.rl_trainer\n",
    "import rob831.infrastructure.utils\n",
    "\n",
    "importlib.reload(rob831.policies.MLP_policy)\n",
    "importlib.reload(rob831.agents.pg_agent)\n",
    "importlib.reload(rob831.infrastructure.rl_trainer)\n",
    "importlib.reload(rob831.infrastructure.utils)\n",
    "\n",
    "from rob831.infrastructure.rl_trainer import RL_Trainer\n",
    "from rob831.agents.pg_agent import PGAgent\n",
    "\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Q6NaOWhOinnU"
   },
   "outputs": [],
   "source": [
    "#@title runtime arguments\n",
    "\n",
    "class Args:\n",
    "\n",
    "  def __getitem__(self, key):\n",
    "    return getattr(self, key)\n",
    "\n",
    "  def __setitem__(self, key, val):\n",
    "    setattr(self, key, val)\n",
    "\n",
    "  def __contains__(self, key):\n",
    "    return hasattr(self, key)\n",
    "\n",
    "  env_name = 'CartPole-v0' #@param\n",
    "  exp_name = 'q1_sb_rtg_na' #@param\n",
    "\n",
    "  #@markdown main parameters of interest\n",
    "  n_iter = 100 #@param {type: \"integer\"}\n",
    "\n",
    "  ## PDF will tell you how to set ep_len\n",
    "  ## and discount for each environment\n",
    "  ep_len = 200 #@param {type: \"integer\"}\n",
    "  discount = 0.95 #@param {type: \"number\"}\n",
    "\n",
    "  reward_to_go = True #@param {type: \"boolean\"}\n",
    "  nn_baseline = False #@param {type: \"boolean\"}\n",
    "  gae_lambda = None #@param {type: \"number\"}\n",
    "  dont_standardize_advantages = False #@param {type: \"boolean\"}\n",
    "\n",
    "  #@markdown batches and steps\n",
    "  batch_size = 1000 #@param {type: \"integer\"}\n",
    "  eval_batch_size = 400 #@param {type: \"integer\"}\n",
    "\n",
    "  num_agent_train_steps_per_iter = 1 #@param {type: \"integer\"}\n",
    "  learning_rate =  5e-3 #@param {type: \"number\"}\n",
    "\n",
    "  #@markdown MLP parameters\n",
    "  n_layers = 2 #@param {type: \"integer\"}\n",
    "  size = 64 #@param {type: \"integer\"}\n",
    "\n",
    "  #@markdown system\n",
    "  save_params = False #@param {type: \"boolean\"}\n",
    "  no_gpu = False #@param {type: \"boolean\"}\n",
    "  which_gpu = 0 #@param {type: \"integer\"}\n",
    "  seed = 1 #@param {type: \"integer\"}\n",
    "\n",
    "  action_noise_std = 0 #@param {type: \"number\"}\n",
    "\n",
    "  #@markdown logging\n",
    "  ## default is to not log video so\n",
    "  ## that logs are small enough to be\n",
    "  ## uploaded to gradscope\n",
    "  video_log_freq =  -1#@param {type: \"integer\"}\n",
    "  scalar_log_freq =  1#@param {type: \"integer\"}\n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n",
    "## ensure compatibility with hw1 code\n",
    "args['train_batch_size'] = args['batch_size']\n",
    "\n",
    "if args['video_log_freq'] > 0:\n",
    "  import warnings\n",
    "  warnings.warn(\n",
    "      '''\\nLogging videos will make eventfiles too'''\n",
    "      '''\\nlarge for the autograder. Set video_log_freq = -1'''\n",
    "      '''\\nfor the runs you intend to submit.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "eScWwHhnsYkd"
   },
   "outputs": [],
   "source": [
    "#@title create directory for logging\n",
    "\n",
    "data_path = '''/content/hw_16831/hw2/data'''\n",
    "\n",
    "if not (os.path.exists(data_path)):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "logdir = args.exp_name + '_' + args.env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "logdir = os.path.join(data_path, logdir)\n",
    "args['logdir'] = logdir\n",
    "if not(os.path.exists(logdir)):\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "aljzrLdAsvNu"
   },
   "outputs": [],
   "source": [
    "## define policy gradient trainer\n",
    "\n",
    "class PG_Trainer(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "\n",
    "        #####################\n",
    "        ## SET AGENT PARAMS\n",
    "        #####################\n",
    "\n",
    "        computation_graph_args = {\n",
    "            'n_layers': params['n_layers'],\n",
    "            'size': params['size'],\n",
    "            'learning_rate': params['learning_rate'],\n",
    "            }\n",
    "\n",
    "        estimate_advantage_args = {\n",
    "            'gamma': params['discount'],\n",
    "            'standardize_advantages': not(params['dont_standardize_advantages']),\n",
    "            'reward_to_go': params['reward_to_go'],\n",
    "            'nn_baseline': params['nn_baseline'],\n",
    "            'gae_lambda': params['gae_lambda'],\n",
    "        }\n",
    "\n",
    "        train_args = {\n",
    "            'num_agent_train_steps_per_iter': params['num_agent_train_steps_per_iter'],\n",
    "        }\n",
    "\n",
    "        agent_params = {**computation_graph_args, **estimate_advantage_args, **train_args}\n",
    "\n",
    "        self.params = params\n",
    "        self.params['agent_class'] = PGAgent\n",
    "        self.params['agent_params'] = agent_params\n",
    "        self.params['batch_size_initial'] = self.params['batch_size']\n",
    "\n",
    "        ################\n",
    "        ## RL TRAINER\n",
    "        ################\n",
    "\n",
    "        self.rl_trainer = RL_Trainer(self.params)\n",
    "\n",
    "    def run_training_loop(self):\n",
    "\n",
    "        self.rl_trainer.run_training_loop(\n",
    "            self.params['n_iter'],\n",
    "            collect_policy = self.rl_trainer.agent.actor,\n",
    "            eval_policy = self.rl_trainer.agent.actor,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j2rCuQsRsd3N"
   },
   "outputs": [],
   "source": [
    "# Before running the below cell for Q3 (LunarLander), please modify lunar_lander\n",
    "\n",
    "## run training\n",
    "#@title imports\n",
    "import importlib\n",
    "import os\n",
    "import time\n",
    "import rob831.policies.MLP_policy\n",
    "import rob831.agents.pg_agent\n",
    "import rob831.infrastructure.rl_trainer\n",
    "import rob831.infrastructure.utils\n",
    "\n",
    "# path numpy for newer version\n",
    "import numpy as np\n",
    "if not hasattr(np, \"bool8\"):\n",
    "    np.bool8 = np.bool_\n",
    "\n",
    "import gym.envs.box2d.lunar_lander as lunar_lander\n",
    "\n",
    "# auto-reload lunar lander file after modifying\n",
    "importlib.reload(lunar_lander)\n",
    "\n",
    "importlib.reload(rob831.policies.MLP_policy)\n",
    "importlib.reload(rob831.agents.pg_agent)\n",
    "importlib.reload(rob831.infrastructure.rl_trainer)\n",
    "importlib.reload(rob831.infrastructure.utils)\n",
    "\n",
    "from rob831.infrastructure.rl_trainer import RL_Trainer\n",
    "from rob831.agents.pg_agent import PGAgent\n",
    "\n",
    "print(args.logdir)\n",
    "trainer = PG_Trainer(args)\n",
    "trainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "km7LlYvhqKTl"
   },
   "outputs": [],
   "source": [
    "#@markdown You can visualize your runs with tensorboard from within the notebook\n",
    "\n",
    "## requires tensorflow==2.3.0\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/hw_16831/hw2/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Assignment experiment run order\n1. **CartPole-v0 variance analysis**\n   ```bash\n   python rob831/scripts/run_hw2.py --env_name CartPole-v0 -n 150 -b 1500 -dsa --exp_name q1_sb_no_rtg_dsa\n   python rob831/scripts/run_hw2.py --env_name CartPole-v0 -n 150 -b 1500 -rtg -dsa --exp_name q1_sb_rtg_dsa\n   python rob831/scripts/run_hw2.py --env_name CartPole-v0 -n 150 -b 1500 -rtg --exp_name q1_sb_rtg_na\n   python rob831/scripts/run_hw2.py --env_name CartPole-v0 -n 150 -b 6000 -dsa --exp_name q1_lb_no_rtg_dsa\n   python rob831/scripts/run_hw2.py --env_name CartPole-v0 -n 150 -b 6000 -rtg -dsa --exp_name q1_lb_rtg_dsa\n   python rob831/scripts/run_hw2.py --env_name CartPole-v0 -n 150 -b 6000 -rtg --exp_name q1_lb_rtg_na\n   ```\n2. **InvertedPendulum-v4 hyperparameter sweep**\n   ```bash\n   python rob831/scripts/run_hw2.py --env_name InvertedPendulum-v4 \\\n       --ep_len 1000 --discount 0.92 -n 100 -l 2 -s 64 -b <b*> -lr <r*> -rtg \\\n       --exp_name q2_b<b*>_r<r*>\n   ```\n   > Replace `<b*>`/`<r*>` with the sweep values you test; record the smallest batch size and largest learning rate that solve the task in the PDF.\n3. **LunarLanderContinuous-v4 baseline run**\n   ```bash\n   python rob831/scripts/run_hw2.py \\\n       --env_name LunarLanderContinuous-v4 --ep_len 1000 \\\n       --discount 0.99 -n 100 -l 2 -s 64 -b 10000 -lr 0.005 \\\n       --reward_to_go --nn_baseline --exp_name q3_b10000_r0.005\n   ```\n4. **HalfCheetah-v4 comparisons**\n   ```bash\n   python rob831/scripts/run_hw2.py --env_name HalfCheetah-v4 --ep_len 150 \\\n       --discount 0.95 -n 100 -l 2 -s 32 -b 10000 -lr 0.02 \\\n       --exp_name q4_search_b10000_lr0.02\n   python rob831/scripts/run_hw2.py --env_name HalfCheetah-v4 --ep_len 150 \\\n       --discount 0.95 -n 100 -l 2 -s 32 -b 10000 -lr 0.02 -rtg \\\n       --exp_name q4_search_b10000_lr0.02_rtg\n   python rob831/scripts/run_hw2.py --env_name HalfCheetah-v4 --ep_len 150 \\\n       --discount 0.95 -n 100 -l 2 -s 32 -b 10000 -lr 0.02 --nn_baseline \\\n       --exp_name q4_search_b10000_lr0.02_nnbaseline\n   python rob831/scripts/run_hw2.py --env_name HalfCheetah-v4 --ep_len 150 \\\n       --discount 0.95 -n 100 -l 2 -s 32 -b 10000 -lr 0.02 -rtg --nn_baseline \\\n       --exp_name q4_search_b10000_lr0.02_rtg_nnbaseline\n   ```\n   ```bash\n   python rob831/scripts/run_hw2.py --env_name HalfCheetah-v4 --ep_len 150 \\\n       --discount 0.95 -n 100 -l 2 -s 32 -b <b*> -lr <r*> \\\n       --exp_name q4_b<b*>_r<r*>\n   python rob831/scripts/run_hw2.py --env_name HalfCheetah-v4 --ep_len 150 \\\n       --discount 0.95 -n 100 -l 2 -s 32 -b <b*> -lr <r*> -rtg \\\n       --exp_name q4_b<b*>_r<r*>_rtg\n   python rob831/scripts/run_hw2.py --env_name HalfCheetah-v4 --ep_len 150 \\\n       --discount 0.95 -n 100 -l 2 -s 32 -b <b*> -lr <r*> --nn_baseline \\\n       --exp_name q4_b<b*>_r<r*>_nnbaseline\n   python rob831/scripts/run_hw2.py --env_name HalfCheetah-v4 --ep_len 150 \\\n       --discount 0.95 -n 100 -l 2 -s 32 -b <b*> -lr <r*> -rtg --nn_baseline \\\n       --exp_name q4_b<b*>_r<r*>_rtg_nnbaseline\n   ```\n   > Use the `<b*>`/`<r*>` values you identified as optimal in the previous sweep.\n5. **Hopper-v4 GAE ablation**\n   ```bash\n   # lambda in [0, 0.95, 0.99, 1]\n   python rob831/scripts/run_hw2.py \\\n       --env_name Hopper-v4 --ep_len 1000 \\\n       --discount 0.99 -n 300 -l 2 -s 32 -b 2000 -lr 0.001 \\\n       --reward_to_go --nn_baseline --action_noise_std 0.5 --gae_lambda <lambda> \\\n       --exp_name q5_b2000_r0.001_lambda<lambda>\n   ```\n6. *(Optional bonus)* Parallelization and multi-gradient-step experiments can be launched by repeating `python rob831/scripts/run_hw2.py` with the additional flags described in the PDF.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18ad8508cc244a8699efc58f7a392a25"
   },
   "source": [
    "## Experiment automation and analysis helpers",
    "",
    "The cells below automate the launch of the homework experiments and collect the TensorBoard logs required for the HW2 report. Run them after the setup section completes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5ff1c88e6a054955bb1a3c499fb8bb9c"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Configure homework directories\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "HW2_ROOT = Path.cwd().resolve()\n",
    "DATA_DIR = HW2_ROOT / 'data'\n",
    "print(f\"Working directory: {HW2_ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6122300e297f487da6bcc6da720cce9d"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Helper functions for loading logs and plotting curves\n",
    "import shlex\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError:  # pragma: no cover - pandas is available in Colab but optional\n",
    "    pd = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RunConfig:\n",
    "    label: str\n",
    "    exp_name: str\n",
    "    env_name: str\n",
    "\n",
    "\n",
    "def resolve_run_dirs(exp_name: str, env_name: Optional[str] = None, data_dir: Path = DATA_DIR) -> List[Path]:\n",
    "    pattern = exp_name\n",
    "    if env_name:\n",
    "        pattern += f'_{env_name}'\n",
    "    pattern += '*'\n",
    "    matches = sorted(data_dir.glob(pattern))\n",
    "    return matches\n",
    "\n",
    "\n",
    "def load_event_scalars(run_dir: Path, tag: str = 'Eval_AverageReturn') -> Tuple[np.ndarray, np.ndarray]:\n",
    "    ea = event_accumulator.EventAccumulator(str(run_dir), size_guidance={'scalars': 0})\n",
    "    ea.Reload()\n",
    "    events = ea.Scalars(tag)\n",
    "    steps = np.array([event.step for event in events])\n",
    "    values = np.array([event.value for event in events])\n",
    "    return steps, values\n",
    "\n",
    "\n",
    "def collect_run_data(configs: Sequence[RunConfig], tag: str = 'Eval_AverageReturn', selector: Union[str, int] = 'latest'):\n",
    "    run_data = []\n",
    "    for cfg in configs:\n",
    "        matches = resolve_run_dirs(cfg.exp_name, cfg.env_name)\n",
    "        if not matches:\n",
    "            print(f\"[warn] No runs found for exp_name='{cfg.exp_name}' env_name='{cfg.env_name}'.\")\n",
    "            continue\n",
    "        if selector == 'latest':\n",
    "            run_dir = max(matches, key=lambda p: p.stat().st_mtime)\n",
    "        elif isinstance(selector, int):\n",
    "            run_dir = matches[selector]\n",
    "        else:\n",
    "            run_dir = matches[0]\n",
    "        steps, values = load_event_scalars(run_dir, tag)\n",
    "        run_data.append({\n",
    "            'label': cfg.label,\n",
    "            'exp_name': cfg.exp_name,\n",
    "            'env_name': cfg.env_name,\n",
    "            'run_dir': run_dir,\n",
    "            'steps': steps,\n",
    "            'values': values,\n",
    "            'tag': tag,\n",
    "        })\n",
    "    return run_data\n",
    "\n",
    "\n",
    "def plot_learning_curves(run_data, title: str, ylabel: str = 'Average Return', smoothing_window: Optional[int] = None):\n",
    "    if not run_data:\n",
    "        print('[warn] No run data available to plot.')\n",
    "        return\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for item in run_data:\n",
    "        values = item['values']\n",
    "        steps = item['steps']\n",
    "        if smoothing_window and len(values) >= smoothing_window:\n",
    "            if pd is not None:\n",
    "                smoothed = pd.Series(values).rolling(smoothing_window, min_periods=1).mean().to_numpy()\n",
    "            else:\n",
    "                kernel = np.ones(smoothing_window) / smoothing_window\n",
    "                smoothed = np.convolve(values, kernel, mode='same')\n",
    "            plt.plot(steps, smoothed, label=item['label'])\n",
    "        else:\n",
    "            plt.plot(steps, values, label=item['label'])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(True, linestyle='--', alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def summarize_run_data(run_data):\n",
    "    if not run_data:\n",
    "        return\n",
    "    rows = []\n",
    "    for item in run_data:\n",
    "        values = item['values']\n",
    "        if len(values) == 0:\n",
    "            continue\n",
    "        rows.append({\n",
    "            'label': item['label'],\n",
    "            'exp_name': item['exp_name'],\n",
    "            'env': item['env_name'],\n",
    "            'iterations': int(len(values)),\n",
    "            'final_return': float(values[-1]),\n",
    "            'max_return': float(np.max(values)),\n",
    "            'logdir': item['run_dir'].name,\n",
    "        })\n",
    "    if not rows:\n",
    "        print('[warn] No scalar values logged yet.')\n",
    "        return\n",
    "    if pd is not None:\n",
    "        display(pd.DataFrame(rows))\n",
    "    else:\n",
    "        for row in rows:\n",
    "            print(row)\n",
    "\n",
    "\n",
    "def first_iteration_at_or_above(values: np.ndarray, steps: np.ndarray, threshold: float) -> Optional[int]:\n",
    "    for step, value in zip(steps, values):\n",
    "        if value >= threshold:\n",
    "            return int(step)\n",
    "    return None\n",
    "\n",
    "\n",
    "def report_threshold_crossings(run_data, threshold: float):\n",
    "    for item in run_data:\n",
    "        step = first_iteration_at_or_above(item['values'], item['steps'], threshold)\n",
    "        if step is None:\n",
    "            print(f\"{item['label']}: did not reach {threshold} (max={np.max(item['values']):.1f}).\")\n",
    "        else:\n",
    "            print(f\"{item['label']}: reached {threshold} at iteration {step}.\")\n",
    "\n",
    "\n",
    "def parse_list(value: str, cast=float):\n",
    "    if isinstance(value, (list, tuple)):\n",
    "        return [cast(v) for v in value]\n",
    "    tokens = [token.strip() for token in str(value).split(',') if token.strip()]\n",
    "    return [cast(token) for token in tokens]\n",
    "\n",
    "\n",
    "def extract_flag(tokens: List[str], flag: str) -> Optional[str]:\n",
    "    if flag not in tokens:\n",
    "        return None\n",
    "    index = tokens.index(flag)\n",
    "    if index == len(tokens) - 1:\n",
    "        return None\n",
    "    return tokens[index + 1]\n",
    "\n",
    "\n",
    "def run_experiment_commands(commands: Sequence[str], skip_existing: bool = True):\n",
    "    for cmd in commands:\n",
    "        tokens = shlex.split(cmd)\n",
    "        exp_name = extract_flag(tokens, '--exp_name')\n",
    "        env_name = extract_flag(tokens, '--env_name')\n",
    "        print(f\"\n",
    ">>> {cmd}\")\n",
    "        if skip_existing and exp_name and env_name and resolve_run_dirs(exp_name, env_name):\n",
    "            print(f\"[skip] Logs already exist for {exp_name} ({env_name}). Delete the folder in {DATA_DIR} to rerun.\")\n",
    "            continue\n",
    "        subprocess.run(cmd, shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1c50a9e7e0f44d73836561646cc3fbef"
   },
   "source": [
    "## Experiment 1 (CartPole-v0): variance analysis",
    "",
    "Run the small- and large-batch experiments and then generate the plots required for the report."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "e2b076b472c54da58dfd3b28ee9510e8"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Run CartPole small-batch experiments\n",
    "cartpole_small_commands = [\n",
    "    \"python rob831/scripts/run_hw2.py --env_name CartPole-v0 -n 150 -b 1500 -dsa --exp_name q1_sb_no_rtg_dsa\",\n",
    "    \"python rob831/scripts/run_hw2.py --env_name CartPole-v0 -n 150 -b 1500 -rtg -dsa --exp_name q1_sb_rtg_dsa\",\n",
    "    \"python rob831/scripts/run_hw2.py --env_name CartPole-v0 -n 150 -b 1500 -rtg --exp_name q1_sb_rtg_na\",\n",
    "]\n",
    "run_experiment_commands(cartpole_small_commands)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ed7d7c60fca445c09ea5a8c122157711"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Run CartPole large-batch experiments\n",
    "cartpole_large_commands = [\n",
    "    \"python rob831/scripts/run_hw2.py --env_name CartPole-v0 -n 150 -b 6000 -dsa --exp_name q1_lb_no_rtg_dsa\",\n",
    "    \"python rob831/scripts/run_hw2.py --env_name CartPole-v0 -n 150 -b 6000 -rtg -dsa --exp_name q1_lb_rtg_dsa\",\n",
    "    \"python rob831/scripts/run_hw2.py --env_name CartPole-v0 -n 150 -b 6000 -rtg --exp_name q1_lb_rtg_na\",\n",
    "]\n",
    "run_experiment_commands(cartpole_large_commands)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "418fa3facccf4763b1c06a816b04c8ff"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Plot CartPole learning curves\n",
    "q1_tag = 'Eval_AverageReturn'  #@param [\"Eval_AverageReturn\", \"Train_AverageReturn\"]\n",
    "small_configs = [\n",
    "    RunConfig('No RTG, standardized advantages', 'q1_sb_no_rtg_dsa', 'CartPole-v0'),\n",
    "    RunConfig('RTG + standardized advantages', 'q1_sb_rtg_dsa', 'CartPole-v0'),\n",
    "    RunConfig('RTG only', 'q1_sb_rtg_na', 'CartPole-v0'),\n",
    "]\n",
    "large_configs = [\n",
    "    RunConfig('No RTG, standardized advantages', 'q1_lb_no_rtg_dsa', 'CartPole-v0'),\n",
    "    RunConfig('RTG + standardized advantages', 'q1_lb_rtg_dsa', 'CartPole-v0'),\n",
    "    RunConfig('RTG only', 'q1_lb_rtg_na', 'CartPole-v0'),\n",
    "]\n",
    "small_data = collect_run_data(small_configs, tag=q1_tag)\n",
    "plot_learning_curves(small_data, title=f'CartPole-v0 (small batch, tag={q1_tag})', ylabel=q1_tag.replace('_', ' '))\n",
    "summarize_run_data(small_data)\n",
    "large_data = collect_run_data(large_configs, tag=q1_tag)\n",
    "plot_learning_curves(large_data, title=f'CartPole-v0 (large batch, tag={q1_tag})', ylabel=q1_tag.replace('_', ' '))\n",
    "summarize_run_data(large_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8c076795aebc4a669e45b63b9ca18220"
   },
   "source": [
    "## Experiment 2 (InvertedPendulum-v4): hyperparameter sweep",
    "",
    "Use the cell below to sweep over batch sizes and learning rates. Adjust the comma-separated lists as needed to search for the optimal configuration."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bb3eec4fa1e444f290875cdf52241f23"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Run InvertedPendulum sweep\n",
    "batch_sizes = \"2000, 4000, 6000\"  #@param {type:\"string\"}\n",
    "learning_rates = \"0.005, 0.01, 0.02\"  #@param {type:\"string\"}\n",
    "\n",
    "b_values = parse_list(batch_sizes, cast=float)\n",
    "r_values = parse_list(learning_rates, cast=float)\n",
    "\n",
    "sweep_commands = []\n",
    "for b in b_values:\n",
    "    for lr in r_values:\n",
    "        exp = f\"q2_b{int(b)}_r{lr}\"\n",
    "        cmd = (\n",
    "            \"python rob831/scripts/run_hw2.py --env_name InvertedPendulum-v4 \"\n",
    "            \"--ep_len 1000 --discount 0.92 -n 100 -l 2 -s 64 \"\n",
    "            f\"-b {int(b)} -lr {lr} -rtg --exp_name {exp}\"\n",
    "        )\n",
    "        sweep_commands.append(cmd)\n",
    "\n",
    "run_experiment_commands(sweep_commands)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6f04ae0dec05418f95acccb92e2f8665"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Plot the best InvertedPendulum run\n",
    "best_batch_size = 2000  #@param {type:\"integer\"}\n",
    "best_learning_rate = 0.01  #@param {type:\"number\"}\n",
    "q2_tag = 'Eval_AverageReturn'  #@param [\"Eval_AverageReturn\", \"Train_AverageReturn\"]\n",
    "\n",
    "best_exp = f\"q2_b{best_batch_size}_r{best_learning_rate}\"\n",
    "q2_data = collect_run_data([RunConfig('Selected hyperparameters', best_exp, 'InvertedPendulum-v4')], tag=q2_tag)\n",
    "plot_learning_curves(q2_data, title=f'InvertedPendulum-v4 ({best_exp})', ylabel=q2_tag.replace('_', ' '))\n",
    "summarize_run_data(q2_data)\n",
    "if q2_data:\n",
    "    report_threshold_crossings(q2_data, threshold=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb51b0d986864992bf1f42d20b00473b"
   },
   "source": [
    "## Experiment 3 (LunarLanderContinuous-v4): baseline run",
    "",
    "Launch the baseline run and then visualize its learning curve."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9374652cecd14c8c880898ed6f958998"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Run LunarLander baseline experiment\n",
    "lunarlander_commands = [\n",
    "    \"python rob831/scripts/run_hw2.py --env_name LunarLanderContinuous-v4 --ep_len 1000 \"\n",
    "    \"--discount 0.99 -n 100 -l 2 -s 64 -b 10000 -lr 0.005 --reward_to_go --nn_baseline --exp_name q3_b10000_r0.005\"\n",
    "]\n",
    "run_experiment_commands(lunarlander_commands)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3babde9bda054fb986b6b17abad22266"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Plot LunarLander baseline curve\n",
    "q3_tag = 'Eval_AverageReturn'  #@param [\"Eval_AverageReturn\", \"Train_AverageReturn\"]\n",
    "lunar_data = collect_run_data([RunConfig('LunarLander baseline', 'q3_b10000_r0.005', 'LunarLanderContinuous-v4')], tag=q3_tag)\n",
    "plot_learning_curves(lunar_data, title='LunarLanderContinuous-v4 baseline', ylabel=q3_tag.replace('_', ' '))\n",
    "summarize_run_data(lunar_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a505aaed2f6440ce8770b113b4c63e3b"
   },
   "source": [
    "## Experiment 4 (HalfCheetah-v4): policy gradient comparisons",
    "",
    "The following cells cover both the provided baseline configuration sweep and the follow-up experiments with the optimal hyperparameters you identify."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "75721dac43174e5a9c51d9c4904f6933"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Run HalfCheetah baseline comparison experiments\n",
    "halfcheetah_baseline_commands = [\n",
    "    \"python rob831/scripts/run_hw2.py --env_name HalfCheetah-v4 --ep_len 150 --discount 0.95 -n 100 -l 2 -s 32 -b 10000 -lr 0.02 --exp_name q4_search_b10000_lr0.02\",\n",
    "    \"python rob831/scripts/run_hw2.py --env_name HalfCheetah-v4 --ep_len 150 --discount 0.95 -n 100 -l 2 -s 32 -b 10000 -lr 0.02 -rtg --exp_name q4_search_b10000_lr0.02_rtg\",\n",
    "    \"python rob831/scripts/run_hw2.py --env_name HalfCheetah-v4 --ep_len 150 --discount 0.95 -n 100 -l 2 -s 32 -b 10000 -lr 0.02 --nn_baseline --exp_name q4_search_b10000_lr0.02_nnbaseline\",\n",
    "    \"python rob831/scripts/run_hw2.py --env_name HalfCheetah-v4 --ep_len 150 --discount 0.95 -n 100 -l 2 -s 32 -b 10000 -lr 0.02 -rtg --nn_baseline --exp_name q4_search_b10000_lr0.02_rtg_nnbaseline\",\n",
    "]\n",
    "run_experiment_commands(halfcheetah_baseline_commands)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "acef9ca0548b499795dfaa870a9d36f2"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Plot HalfCheetah baseline comparisons\n",
    "q4_tag = 'Eval_AverageReturn'  #@param [\"Eval_AverageReturn\", \"Train_AverageReturn\"]\n",
    "baseline_configs = [\n",
    "    RunConfig('Baseline (no RTG / no baseline)', 'q4_search_b10000_lr0.02', 'HalfCheetah-v4'),\n",
    "    RunConfig('Baseline + RTG', 'q4_search_b10000_lr0.02_rtg', 'HalfCheetah-v4'),\n",
    "    RunConfig('Baseline + NN baseline', 'q4_search_b10000_lr0.02_nnbaseline', 'HalfCheetah-v4'),\n",
    "    RunConfig('Baseline + RTG + NN baseline', 'q4_search_b10000_lr0.02_rtg_nnbaseline', 'HalfCheetah-v4'),\n",
    "]\n",
    "baseline_data = collect_run_data(baseline_configs, tag=q4_tag)\n",
    "plot_learning_curves(baseline_data, title='HalfCheetah-v4 baseline comparisons', ylabel=q4_tag.replace('_', ' '))\n",
    "summarize_run_data(baseline_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5a2b2e5f5fec49fb82c318835206a393"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Run HalfCheetah hyperparameter sweep\n",
    "halfcheetah_batch_sizes = \"5000, 7500, 10000\"  #@param {type:\"string\"}\n",
    "halfcheetah_learning_rates = \"0.01, 0.02, 0.03\"  #@param {type:\"string\"}\n",
    "\n",
    "hc_b_values = parse_list(halfcheetah_batch_sizes, cast=float)\n",
    "hc_lr_values = parse_list(halfcheetah_learning_rates, cast=float)\n",
    "\n",
    "hc_sweep_commands = []\n",
    "for b in hc_b_values:\n",
    "    for lr in hc_lr_values:\n",
    "        exp = f\"q4_b{int(b)}_r{lr}\"\n",
    "        base_cmd = (\n",
    "            \"python rob831/scripts/run_hw2.py --env_name HalfCheetah-v4 --ep_len 150 --discount 0.95 -n 100 -l 2 -s 32 \"\n",
    "            f\"-b {int(b)} -lr {lr}\"\n",
    "        )\n",
    "        hc_sweep_commands.extend([\n",
    "            f\"{base_cmd} --exp_name {exp}\",\n",
    "            f\"{base_cmd} -rtg --exp_name {exp}_rtg\",\n",
    "            f\"{base_cmd} --nn_baseline --exp_name {exp}_nnbaseline\",\n",
    "            f\"{base_cmd} -rtg --nn_baseline --exp_name {exp}_rtg_nnbaseline\",\n",
    "        ])\n",
    "\n",
    "run_experiment_commands(hc_sweep_commands)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ca62e4e7d10a4bd189bbeb4ceea8660b"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Plot HalfCheetah runs with selected b* and r*\n",
    "optimal_batch_size = 10000  #@param {type:\"integer\"}\n",
    "optimal_learning_rate = 0.02  #@param {type:\"number\"}\n",
    "q4_opt_tag = 'Eval_AverageReturn'  #@param [\"Eval_AverageReturn\", \"Train_AverageReturn\"]\n",
    "\n",
    "opt_prefix = f\"q4_b{optimal_batch_size}_r{optimal_learning_rate}\"\n",
    "optimal_configs = [\n",
    "    RunConfig('No RTG / no baseline', opt_prefix, 'HalfCheetah-v4'),\n",
    "    RunConfig('+ RTG', f\"{opt_prefix}_rtg\", 'HalfCheetah-v4'),\n",
    "    RunConfig('+ NN baseline', f\"{opt_prefix}_nnbaseline\", 'HalfCheetah-v4'),\n",
    "    RunConfig('+ RTG + NN baseline', f\"{opt_prefix}_rtg_nnbaseline\", 'HalfCheetah-v4'),\n",
    "]\n",
    "optimal_data = collect_run_data(optimal_configs, tag=q4_opt_tag)\n",
    "plot_learning_curves(optimal_data, title=f'HalfCheetah-v4 (b*={optimal_batch_size}, r*={optimal_learning_rate})', ylabel=q4_opt_tag.replace('_', ' '))\n",
    "summarize_run_data(optimal_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaa6e869eb924b21b80a25ca686e4768"
   },
   "source": [
    "## Experiment 5 (Hopper-v4): GAE ablation",
    "",
    "Run the four GAE settings and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9f284c9a7e7642049dedc5d9b8647e55"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Run Hopper GAE experiments\n",
    "gae_lambdas = \"0.0, 0.95, 0.99, 1.0\"  #@param {type:\"string\"}\n",
    "\n",
    "lambda_values = parse_list(gae_lambdas, cast=float)\n",
    "hopper_commands = []\n",
    "for lam in lambda_values:\n",
    "    exp = f\"q5_b2000_r0.001_lambda{lam}\"\n",
    "    cmd = (\n",
    "        \"python rob831/scripts/run_hw2.py --env_name Hopper-v4 --ep_len 1000 --discount 0.99 \"\n",
    "        \"-n 300 -l 2 -s 32 -b 2000 -lr 0.001 --reward_to_go --nn_baseline --action_noise_std 0.5 \"\n",
    "        f\"--gae_lambda {lam} --exp_name {exp}\"\n",
    "    )\n",
    "    hopper_commands.append(cmd)\n",
    "\n",
    "run_experiment_commands(hopper_commands)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dea9080c86a440f7a8120d2c7a3e7982"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Plot Hopper GAE learning curves\n",
    "q5_tag = 'Eval_AverageReturn'  #@param [\"Eval_AverageReturn\", \"Train_AverageReturn\"]\n",
    "\n",
    "lambda_values_for_plot = lambda_values if 'lambda_values' in globals() else parse_list('0.0,0.95,0.99,1.0', cast=float)\n",
    "\n",
    "gae_configs = [RunConfig(f\"lambda={lam}\", f\"q5_b2000_r0.001_lambda{lam}\", 'Hopper-v4') for lam in lambda_values_for_plot]\n",
    "gae_data = collect_run_data(gae_configs, tag=q5_tag)\n",
    "plot_learning_curves(gae_data, title='Hopper-v4 GAE comparison', ylabel=q5_tag.replace('_', ' '), smoothing_window=5)\n",
    "summarize_run_data(gae_data)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}